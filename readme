Normality tests are very important in statistical inference; their purpose is to know if the data is sampled from Normal population. The normality of the data is a prerequisite for several parametric statistics such as t-test, ANOVA, and regression analysis. Violation to the normality assumption may yield to incorrect results and wrong decisions. There are many normality tests exist that can be used to detect if the sample data comes from “non-normal” underlying distribution. But these tests sometimes lead to contradicting results and some of them can be applied under certain conditions. The main goal of this research is to use the machine leaning techniques to create a model based test that could be of a good quality compared to the existing tests.
 
This research tries to build a machine learning classification model to predict the “normality” of the data using three classification techniques: Random Forest (RF), Gradient Boosting Machines (GBM), and Support Vector Machines (SVM). Size, skewness, kurtosis, median, and percentage of data lies within 1, 2, and 3 standard deviations are the features used in training. The evaluation phase showed similar results for the three models with high accuracy and ROC_AUC values on different test sets with few points in favor for “RF” model.

The new normality test generated with “RF” was compared with seven popular normality tests: Shapiro-Wilk (SW), Anderson-Darling (AD), Jarque-Bera (JB), Shapiro-Francia (SF), Kolmogorov-Smirnov (KS), Cramer-von Mises (CVM), and Lilliefors (Lillie). Monte Carlo simulation on 25 alternative distributions on different sample sizes concluded and the results showed significantly the higher power for the model comparing to the other normality tests.
